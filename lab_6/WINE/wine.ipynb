{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pydot\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.metrics import accuracy_score,\\\n",
    "                            confusion_matrix, \\\n",
    "                            precision_score, \\\n",
    "                            recall_score, \\\n",
    "                            f1_score, \\\n",
    "                            classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "raw_dataset = load_wine()\n",
    "X = raw_dataset[\"data\"]\n",
    "y = raw_dataset[\"target\"]\n",
    "feature_names = raw_dataset[\"feature_names\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data=X, columns=feature_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 13 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       178 non-null    float64\n",
      " 1   malic_acid                    178 non-null    float64\n",
      " 2   ash                           178 non-null    float64\n",
      " 3   alcalinity_of_ash             178 non-null    float64\n",
      " 4   magnesium                     178 non-null    float64\n",
      " 5   total_phenols                 178 non-null    float64\n",
      " 6   flavanoids                    178 non-null    float64\n",
      " 7   nonflavanoid_phenols          178 non-null    float64\n",
      " 8   proanthocyanins               178 non-null    float64\n",
      " 9   color_intensity               178 non-null    float64\n",
      " 10  hue                           178 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  178 non-null    float64\n",
      " 12  proline                       178 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 18.2 KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\ncount  178.000000  178.000000  178.000000         178.000000  178.000000   \nmean    13.000618    2.336348    2.366517          19.494944   99.741573   \nstd      0.811827    1.117146    0.274344           3.339564   14.282484   \nmin     11.030000    0.740000    1.360000          10.600000   70.000000   \n25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n75%     13.677500    3.082500    2.557500          21.500000  107.000000   \nmax     14.830000    5.800000    3.230000          30.000000  162.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount     178.000000  178.000000            178.000000       178.000000   \nmean        2.295112    2.029270              0.361854         1.590899   \nstd         0.625851    0.998859              0.124453         0.572359   \nmin         0.980000    0.340000              0.130000         0.410000   \n25%         1.742500    1.205000              0.270000         1.250000   \n50%         2.355000    2.135000              0.340000         1.555000   \n75%         2.800000    2.875000              0.437500         1.950000   \nmax         3.880000    5.080000              0.660000         3.580000   \n\n       color_intensity         hue  od280/od315_of_diluted_wines      proline  \ncount       178.000000  178.000000                    178.000000   178.000000  \nmean          5.058090    0.957449                      2.611685   746.893258  \nstd           2.318286    0.228572                      0.709990   314.907474  \nmin           1.280000    0.480000                      1.270000   278.000000  \n25%           3.220000    0.782500                      1.937500   500.500000  \n50%           4.690000    0.965000                      2.780000   673.500000  \n75%           6.200000    1.120000                      3.170000   985.000000  \nmax          13.000000    1.710000                      4.000000  1680.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>13.000618</td>\n      <td>2.336348</td>\n      <td>2.366517</td>\n      <td>19.494944</td>\n      <td>99.741573</td>\n      <td>2.295112</td>\n      <td>2.029270</td>\n      <td>0.361854</td>\n      <td>1.590899</td>\n      <td>5.058090</td>\n      <td>0.957449</td>\n      <td>2.611685</td>\n      <td>746.893258</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.811827</td>\n      <td>1.117146</td>\n      <td>0.274344</td>\n      <td>3.339564</td>\n      <td>14.282484</td>\n      <td>0.625851</td>\n      <td>0.998859</td>\n      <td>0.124453</td>\n      <td>0.572359</td>\n      <td>2.318286</td>\n      <td>0.228572</td>\n      <td>0.709990</td>\n      <td>314.907474</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>11.030000</td>\n      <td>0.740000</td>\n      <td>1.360000</td>\n      <td>10.600000</td>\n      <td>70.000000</td>\n      <td>0.980000</td>\n      <td>0.340000</td>\n      <td>0.130000</td>\n      <td>0.410000</td>\n      <td>1.280000</td>\n      <td>0.480000</td>\n      <td>1.270000</td>\n      <td>278.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>12.362500</td>\n      <td>1.602500</td>\n      <td>2.210000</td>\n      <td>17.200000</td>\n      <td>88.000000</td>\n      <td>1.742500</td>\n      <td>1.205000</td>\n      <td>0.270000</td>\n      <td>1.250000</td>\n      <td>3.220000</td>\n      <td>0.782500</td>\n      <td>1.937500</td>\n      <td>500.500000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13.050000</td>\n      <td>1.865000</td>\n      <td>2.360000</td>\n      <td>19.500000</td>\n      <td>98.000000</td>\n      <td>2.355000</td>\n      <td>2.135000</td>\n      <td>0.340000</td>\n      <td>1.555000</td>\n      <td>4.690000</td>\n      <td>0.965000</td>\n      <td>2.780000</td>\n      <td>673.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>13.677500</td>\n      <td>3.082500</td>\n      <td>2.557500</td>\n      <td>21.500000</td>\n      <td>107.000000</td>\n      <td>2.800000</td>\n      <td>2.875000</td>\n      <td>0.437500</td>\n      <td>1.950000</td>\n      <td>6.200000</td>\n      <td>1.120000</td>\n      <td>3.170000</td>\n      <td>985.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>14.830000</td>\n      <td>5.800000</td>\n      <td>3.230000</td>\n      <td>30.000000</td>\n      <td>162.000000</td>\n      <td>3.880000</td>\n      <td>5.080000</td>\n      <td>0.660000</td>\n      <td>3.580000</td>\n      <td>13.000000</td>\n      <td>1.710000</td>\n      <td>4.000000</td>\n      <td>1680.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "alcohol                         126\nmalic_acid                      133\nash                              79\nalcalinity_of_ash                63\nmagnesium                        53\ntotal_phenols                    97\nflavanoids                      132\nnonflavanoid_phenols             39\nproanthocyanins                 101\ncolor_intensity                 132\nhue                              78\nod280/od315_of_diluted_wines    122\nproline                         121\ndtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.nunique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeClassifier()"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "tree.export_graphviz(clf, out_file=\"tree.dot\", filled=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, clf.predict(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nWe can use stratification: with it, we can preserve the original label distribution.\\nThe stratify parameter requires the array that defines, for each individual,\\nthe subpopulation to which it belongs (in our case, our label).'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataset, y, test_size=.2, stratify=y)\n",
    "\n",
    "'''\n",
    "We can use stratification: with it, we can preserve the original label distribution.\n",
    "The stratify parameter requires the array that defines, for each individual,\n",
    "the subpopulation to which it belongs (in our case, our label).'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "clf_v1 = DecisionTreeClassifier()\n",
    "clf_v1.fit(x_train, y_train)\n",
    "print(accuracy_score(y_test, clf_v1.predict(x_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[12  0  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0 10]]\n",
      "Precision score:  [1. 1. 1.]\n",
      "Recall score:  [1. 1. 1.]\n",
      "F1 score:  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "the accuracy tells us about the \"overall\" classifier performance.\n",
    "If we want to know more detailed information (e.g. which classes we are getting right most of the time,\n",
    "or what kind of errors we are typically doing), we can resort to other metrics.\n",
    "'''\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "print('Confusion matrix: \\n',confusion_matrix(y_test, y_pred))\n",
    "print('Precision score: ',precision_score(y_test, y_pred, average=None))\n",
    "print('Recall score: ',recall_score(y_test, y_pred, average=None))\n",
    "print('F1 score: ',f1_score(y_test, y_pred, average=None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We can also use classification_report() to get all of the previous information into a single report\n",
    "(useful for an overview of the classifier performance).\n",
    "'''\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9444444444444444"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": [None, 2, 3, 4, 5],\n",
    "    \"min_impurity_decrease\": [0, .01, .03, .07, .09, .11]\n",
    "}\n",
    "\n",
    "accuracies = []\n",
    "for config in ParameterGrid(params):\n",
    "    clf = DecisionTreeClassifier(**config)\n",
    "    clf.fit(x_train, y_train)\n",
    "    accuracies.append(accuracy_score(y_test, clf.predict(x_test)))\n",
    "max(accuracies)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nNotice how we are computing the final accuracy not as the mean of the accuracies on each fold, \\nbut rather as a weighted average (using np.average with the weights parameter), \\nwhere the weight is the number of elements in each partition. This is the correct way of computing \\nthe average accuracy when each fold might have a different number of elements. Intuitively, \\nthis is because we want larger folds to have a larger contribution to the final results. \\nMore practically, one can prove that computing the weighted average is the same as counting the \\noverall number of correct predictions and dividing by the total number of elements \\n(since each point is only used once for the validation, we can talk about the \"overall accuracy\" \\non X_train_valid).\\n\\nSo now we can get the best performing configuration (through argmax()), train a classifier on \\nall X_train_valid (since the cross-validation has already occurred, we can now use all the data \\navailable for the new training). Finally, we can get our performance estimate on new data to \\nactually know how good our classifier is.\\n'"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In the previous exercise, you searched for the best configuration among a list of possible alternatives.\n",
    "Since we used our test data to select the model, you may be overfitting on the test data (you may\n",
    "have selected the configuration that works best for the test set, but which may not be as good on\n",
    "new data). Typically, you do not want to use the test set for tuning the model’s hyperparameters,\n",
    "since the test set should only be used as a final evaluation.\n",
    "For this reason, datasets are typically splitted into\n",
    "• Training set: used to create the model.\n",
    "• Validation set: used to assess how good each configuration of a classifier is.\n",
    "• Test set: used at the end of the hyperparameter tuning, to assess how good our final model is.\n",
    "However, it often happens that only a limited amount of data is available. In these cases, it is wasteful\n",
    "to only use a small fraction of the dataset for the actual training. In these cases, cross-validation can\n",
    "be used to “get rid” of the validation set. You can read more about cross-validation on the course\n",
    "slides. One popular method of is the k-fold cross-validation. In this, the training set is split into k\n",
    "partitions. k 􀀀 1 are used for the training, the other one is used validation. This is repeated until all\n",
    "partitions have been used once for validation.\n",
    "Sklearn offers the KFold class for doing k-fold cross-validation. You can use this class as follows:\n",
    "'''\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, stratify=y)\n",
    "kf = KFold(5)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for config in ParameterGrid(params):\n",
    "    clf_accuracies = []\n",
    "    counts = []\n",
    "\n",
    "    for train_indices, valid_indices in kf.split(X_train_valid):\n",
    "\n",
    "        X_train = X_train_valid[train_indices]\n",
    "        y_train = y_train_valid[train_indices]\n",
    "        X_valid = X_train_valid[valid_indices]\n",
    "        y_valid = y_train_valid[valid_indices]\n",
    "\n",
    "        # keep track of the number of elements in each split\n",
    "        counts.append(len(train_indices))\n",
    "\n",
    "        clf = DecisionTreeClassifier(**config)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        acc = accuracy_score(y_valid, clf.predict(X_valid))\n",
    "        clf_accuracies.append(acc)\n",
    "\n",
    "    accuracies.append(np.average(clf_accuracies, weights=counts))\n",
    "\n",
    "'''\n",
    "Notice how we are computing the final accuracy not as the mean of the accuracies on each fold,\n",
    "but rather as a weighted average (using np.average with the weights parameter),\n",
    "where the weight is the number of elements in each partition. This is the correct way of computing\n",
    "the average accuracy when each fold might have a different number of elements. Intuitively,\n",
    "this is because we want larger folds to have a larger contribution to the final results.\n",
    "More practically, one can prove that computing the weighted average is the same as counting the\n",
    "overall number of correct predictions and dividing by the total number of elements\n",
    "(since each point is only used once for the validation, we can talk about the \"overall accuracy\"\n",
    "on X_train_valid).\n",
    "\n",
    "So now we can get the best performing configuration (through argmax()), train a classifier on\n",
    "all X_train_valid (since the cross-validation has already occurred, we can now use all the data\n",
    "available for the new training). Finally, we can get our performance estimate on new data to\n",
    "actually know how good our classifier is.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9111111111111111"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_config = list(ParameterGrid(params))[np.argmax(accuracies)]\n",
    "\n",
    "clf = DecisionTreeClassifier(**best_config)\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "\n",
    "accuracy_score(y_test, clf.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 9, -2,  6, -2, 12, -2, -2], dtype=int64)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf.tree_.feature\n",
    "# clf.tree_.feature: a list of the features used at each node.\n",
    "# A value of -2 indicates a leaf node (where no split occurs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.65848833, 0.07986111, 0.56968858, 0.        , 0.24489796,\n       0.21875   , 0.        ])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.tree_.impurity\n",
    "\n",
    "# a list of impurity decreases for each node.\n",
    "# These are computed with the following formula\n",
    "# (the following is taken from scikit-learn's documentation\n",
    "# for DecisionTreeClassifier:\n",
    "# N_t / N * (imp - N_t_R / N_t * right_imp - N_t_L / N_t * left_imp)\n",
    "# (where N is the total number of samples, N_t is the number\n",
    "# of samples at the current node, N_t_L is the number of samples in\n",
    "# the left child, and N_t_R is the number of samples in the right child)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "[color_intensity, leaf, flavanoids, leaf, proline, leaf, leaf]"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The lists of feature, impurity and value are the pre-order visits of the tree.\n",
    "Now, to compute the impurity decrease we need to know, for each node, who its children are.\n",
    "This information is not readily available from the pre-order visit\n",
    "'''\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, impurity=0, feature=-2, value=0):\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.impurity = impurity\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"leaf\" if self.feature == -2 else feature_names[self.feature]\n",
    "\n",
    "'''\n",
    "By default, a new Node object will be a leaf (no children, impurity=0 and no feature).\n",
    "We can modify the object's attribute by directly accessing them.\n",
    "\n",
    "Notice that we also define the __repr__ method.\n",
    "This method defines a representation of our object (used, for example, when printing it).\n",
    "In our case, we will use the \"leaf\" string for leaves, and the feature name of\n",
    "the feature used for the split for split nodes.\n",
    "'''\n",
    "\n",
    "# as already mentioned, we only need\n",
    "# the overall count of elements in each node,\n",
    "# not the information for each class.\n",
    "# The following line computes that list\n",
    "values = clf.tree_.value.sum(axis=2).flatten()\n",
    "\n",
    "nodes = [ Node(impurity, feature, value) \\\n",
    "         for impurity, feature, value in \\\n",
    "         zip(clf.tree_.impurity, clf.tree_.feature, values) ]\n",
    "nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "7"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def traverse_tree(nodes, i):\n",
    "    # we use `feature==-2` to stop\n",
    "    # the traversal, as we just reached a leaf\n",
    "    if nodes[i].feature == -2:\n",
    "        # return the offset to sum to\n",
    "        # the caller's \"i\" to get\n",
    "        # the next element (1 = \"skip this element\")\n",
    "        return 1\n",
    "    # next node is the one\n",
    "    # right next to the current one\n",
    "    nodes[i].right = nodes[i+1]\n",
    "    offset_right = traverse_tree(nodes, i+1)\n",
    "    nodes[i].left = nodes[i+offset_right+1]\n",
    "    offset_left = traverse_tree(nodes, i+offset_right+1)\n",
    "    return offset_right + offset_left + 1\n",
    "\n",
    "traverse_tree(nodes, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def pre_order(node, visited):\n",
    "    visited.append(node)\n",
    "    if node.feature==-2:\n",
    "        return\n",
    "    pre_order(node.right, visited)\n",
    "    pre_order(node.left, visited)\n",
    "\n",
    "po_visit = []\n",
    "pre_order(nodes[0], po_visit)\n",
    "print(po_visit == nodes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.26557958363400763, 0, 0.2738611233967271, 0, 0.07706766917293234, 0, 0]"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_order_id(node, tot_size, visited):\n",
    "    left, right = node.left, node.right\n",
    "    if node.feature==-2:\n",
    "        visited.append(0)\n",
    "        return\n",
    "    decr = (node.impurity * node.value \\\n",
    "            - left.impurity * left.value \\\n",
    "            - right.impurity * right.value)/tot_size\n",
    "    visited.append(decr)\n",
    "    pre_order_id(node.right, tot_size, visited)\n",
    "    pre_order_id(node.left, tot_size, visited)\n",
    "\n",
    "impurity_decreases = []\n",
    "pre_order_id(nodes[0], nodes[0].value, impurity_decreases)\n",
    "impurity_decreases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "{'alcohol': 0.0,\n 'malic_acid': 0.0,\n 'ash': 0.0,\n 'alcalinity_of_ash': 0.0,\n 'magnesium': 0.0,\n 'total_phenols': 0.0,\n 'flavanoids': 0.44421314286613267,\n 'nonflavanoid_phenols': 0.0,\n 'proanthocyanins': 0.0,\n 'color_intensity': 0.4307801708541132,\n 'hue': 0.0,\n 'od280/od315_of_diluted_wines': 0.0,\n 'proline': 0.12500668627975395}"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = np.zeros(X.shape[1])\n",
    "for feature, decr in zip(clf.tree_.feature, impurity_decreases):\n",
    "    feature_importances[feature] += decr\n",
    "feature_importances /= feature_importances.sum()\n",
    "dict(zip(feature_names, feature_importances))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9f4e7ab1",
   "language": "python",
   "display_name": "PyCharm (gpt_eval_repl)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}